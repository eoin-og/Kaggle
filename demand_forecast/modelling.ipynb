{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "In this notebook we're going to try to model our data using stacked regressors. We will use lightgbm as our meta-learner and baseline model to see if stacking improves our accuracy. The Symmetric Mean Absolute Percent Error (Smape) is specified as the evaluation metric that should be used.\n",
    "\n",
    "---\n",
    "### Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Lasso, BayesianRidge\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/final_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation set\n",
    "We will use the first three months of 2017 as the validation set to mirror the task at hand (predicting the first three motnhs of 2018). We will remove the rest of the 2017 data to prevent any data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('id', axis=1, inplace=True)\n",
    "train.sales = np.log1p(train.sales.values)\n",
    "\n",
    "train_data = train[train.year != 2017]\n",
    "val_data = train[(train.year == 2017) & (train.month.isin([1, 2, 3]))]\n",
    "\n",
    "train_X = train_data.drop('sales', axis=1)\n",
    "train_y = train_data.sales\n",
    "\n",
    "val_X = val_data.drop('sales', axis=1)\n",
    "val_y = val_data.sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### SMAPE loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_smape(pred, target):\n",
    "    n = len(pred)\n",
    "    non_zero_idx = np.invert((pred == 0) & (target == 0))\n",
    "    pred, target = np.expm1(pred[non_zero_idx]), np.expm1(target[non_zero_idx])\n",
    "    return (200/n)*np.sum( (np.abs(pred - target)) / (np.abs(pred) + np.abs(target)))\n",
    "\n",
    "\n",
    "def lgbm_smape(preds, train_data):\n",
    "    smape = calc_smape(preds, train_data.get_label())\n",
    "    return 'SMAPE', smape, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Function for creating stacked predictions for training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pred_matrix(rgrs, t_X, t_y, v_X, v_y, n_splits=7):\n",
    "\n",
    "    dsize = len(t_X)\n",
    "    step = dsize // n_splits\n",
    "    t_pred_mat = np.zeros((dsize, len(rgrs)))\n",
    "    v_pred_mat = np.zeros((len(v_y), len(rgrs)))\n",
    "    \n",
    "    for i, rgrc in enumerate(rgrs):\n",
    "        print('\\ntraining {}...'.format(i))\n",
    "        start = time.time()\n",
    "        for idx in range(0, dsize - step, step):\n",
    "            rgr = rgrc()\n",
    "            X = t_X[idx:idx+step]\n",
    "            y = t_y[idx:idx+step]\n",
    "\n",
    "            next_X = t_X[idx+step:idx+(step*2)]\n",
    "            next_y= t_y[idx+step:idx+(step*2)]\n",
    "            \n",
    "            rgr.fit(X, y)\n",
    "            pred_y = rgr.predict(next_X)\n",
    "            print('nxt score: ',  calc_smape(pred_y, next_y))           \n",
    "            t_pred_mat[idx+step:idx+(step*2), i] = pred_y \n",
    "            \n",
    "        print('trn score: ', calc_smape(t_pred_mat[step:, i], t_y[step:]))\n",
    "        rgr = rgrc()\n",
    "        rgr.fit(t_X, t_y)\n",
    "        v_pred = rgr.predict(v_X)\n",
    "                          \n",
    "        print('val score: ', calc_smape(v_pred, v_y))\n",
    "        v_pred_mat[:, i] = v_pred\n",
    "        print('time taken: ', time.time() - start)\n",
    "            \n",
    "    return t_pred_mat, v_pred_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "creating first layer predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsize = 183000*2\n",
    "t_X = train_X[-dsize:].reset_index(drop=True)\n",
    "t_y = train_y[-dsize:].reset_index(drop=True)\n",
    "\n",
    "ab = AdaBoostRegressor\n",
    "rf = RandomForestRegressor\n",
    "ls = Lasso\n",
    "br = BayesianRidge\n",
    "et = ExtraTreesRegressor\n",
    "\n",
    "rgrs = [br, ls, ab, rf, et]\n",
    "t_pred_mat, val_pred_mat = create_pred_matrix(rgrs, t_X.fillna(-1), t_y, val_X, val_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple average score:\n",
    "t_avg_pred = t_pred_max.mean(axis=1)\n",
    "v_avg_pred = val_pred_mat.mean(axis=1)\n",
    "step = len(t_X) // 5 \n",
    "\n",
    "print('trn score: ', smape(t_avg_pred[step:], t_y[step:]))\n",
    "print('val score: ', smape(v_avg_pred, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Baseline 1 lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbtrain = lgb.Dataset(data=t_X[step:], label=t_y[step:])\n",
    "lgbval = lgb.Dataset(data=val_X, label=val_y, reference=lgbtrain)\n",
    "\n",
    "lgb_params_ = {'task':'train', 'boosting_type':'gbdt', 'objective':'regression', \n",
    "              'metric': {'rmse'}, 'num_leaves': 50, 'learning_rate': 0.05, \n",
    "              'feature_fraction': 0.8, 'max_depth': 8, 'verbose': -1,\n",
    "              'num_boost_round':1000, 'early_stopping_rounds':70, 'nthread':-1}\n",
    "\n",
    "rgr = lgb.train(lgb_params_, lgbtrain, valid_sets=[lgbtrain, lgbval], \n",
    "                  feval=lgbm_smape, categorical_feature=[0, 1, 2, 3, 4], verbose_eval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### LightGBM predicting on stacked predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbtrain = lgb.Dataset(data=t_pred_max[step:], label=t_y[step:])\n",
    "lgbval = lgb.Dataset(data=val_pred_mat, label=val_y, reference=lgbtrain)\n",
    "\n",
    "lgb_params_ = {'task':'train', 'boosting_type':'gbdt', 'objective':'regression', \n",
    "              'metric': {'rmse'}, 'num_leaves': 50, 'learning_rate': 0.05, \n",
    "              'feature_fraction': 0.8, 'max_depth': 8, 'verbose': -1,\n",
    "              'num_boost_round':1000, 'early_stopping_rounds':70, 'nthread':-1}\n",
    "\n",
    "rgr = lgb.train(lgb_params_, lgbtrain, valid_sets=[lgbtrain, lgbval], \n",
    "                  feval=lgbm_smape, categorical_feature=[0, 1, 2, 3, 4], verbose_eval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### LightGBM predicting on combined data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_t_X = t_X.copy()\n",
    "combined_t_X[['br_pred', 'ls_pred', 'ab_pred', 'rf_pred', 'et_pred']] = pd.DataFrame(t_pred_max)\n",
    "\n",
    "combined_val_X = val_X.copy()\n",
    "combined_val_X[['br_pred', 'ls_pred', 'ab_pred', 'rf_pred', 'et_pred']] = pd.DataFrame(val_pred_mat)\n",
    "\n",
    "lgbtrain = lgb.Dataset(data=combined_t_X[:step], label=t_y[:step])\n",
    "lgbval = lgb.Dataset(data=combined_val_X, label=val_y, reference=lgbtrain)\n",
    "\n",
    "lgb_params_ = {'task':'train', 'boosting_type':'gbdt', 'objective':'regression', \n",
    "              'metric': {'rmse'}, 'num_leaves': 50, 'learning_rate': 0.05, \n",
    "              'feature_fraction': 0.8, 'max_depth': 8, 'verbose': -1,\n",
    "              'num_boost_round':1000, 'early_stopping_rounds':70, 'nthread':-1}\n",
    "\n",
    "rgr = lgb.train(lgb_params_, lgbtrain, valid_sets=[lgbtrain, lgbval], \n",
    "                  feval=lgbm_smape, categorical_feature=[0, 1, 2, 3, 4], verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
